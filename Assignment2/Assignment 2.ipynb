{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4881748d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: selenium in c:\\users\\mysel\\anaconda3\\lib\\site-packages (4.8.2)\n",
      "Requirement already satisfied: certifi>=2021.10.8 in c:\\users\\mysel\\anaconda3\\lib\\site-packages (from selenium) (2021.10.8)\n",
      "Requirement already satisfied: urllib3[socks]~=1.26 in c:\\users\\mysel\\anaconda3\\lib\\site-packages (from selenium) (1.26.9)\n",
      "Requirement already satisfied: trio~=0.17 in c:\\users\\mysel\\anaconda3\\lib\\site-packages (from selenium) (0.22.0)\n",
      "Requirement already satisfied: trio-websocket~=0.9 in c:\\users\\mysel\\anaconda3\\lib\\site-packages (from selenium) (0.9.2)\n",
      "Requirement already satisfied: sniffio in c:\\users\\mysel\\anaconda3\\lib\\site-packages (from trio~=0.17->selenium) (1.2.0)\n",
      "Requirement already satisfied: exceptiongroup>=1.0.0rc9 in c:\\users\\mysel\\anaconda3\\lib\\site-packages (from trio~=0.17->selenium) (1.1.0)\n",
      "Requirement already satisfied: async-generator>=1.9 in c:\\users\\mysel\\anaconda3\\lib\\site-packages (from trio~=0.17->selenium) (1.10)\n",
      "Requirement already satisfied: idna in c:\\users\\mysel\\anaconda3\\lib\\site-packages (from trio~=0.17->selenium) (3.3)\n",
      "Requirement already satisfied: attrs>=19.2.0 in c:\\users\\mysel\\anaconda3\\lib\\site-packages (from trio~=0.17->selenium) (21.4.0)\n",
      "Requirement already satisfied: outcome in c:\\users\\mysel\\anaconda3\\lib\\site-packages (from trio~=0.17->selenium) (1.2.0)\n",
      "Requirement already satisfied: sortedcontainers in c:\\users\\mysel\\anaconda3\\lib\\site-packages (from trio~=0.17->selenium) (2.4.0)\n",
      "Requirement already satisfied: cffi>=1.14 in c:\\users\\mysel\\anaconda3\\lib\\site-packages (from trio~=0.17->selenium) (1.15.0)\n",
      "Requirement already satisfied: pycparser in c:\\users\\mysel\\anaconda3\\lib\\site-packages (from cffi>=1.14->trio~=0.17->selenium) (2.21)\n",
      "Requirement already satisfied: wsproto>=0.14 in c:\\users\\mysel\\anaconda3\\lib\\site-packages (from trio-websocket~=0.9->selenium) (1.2.0)\n",
      "Requirement already satisfied: PySocks!=1.5.7,<2.0,>=1.5.6 in c:\\users\\mysel\\anaconda3\\lib\\site-packages (from urllib3[socks]~=1.26->selenium) (1.7.1)\n",
      "Requirement already satisfied: h11<1,>=0.9.0 in c:\\users\\mysel\\anaconda3\\lib\\site-packages (from wsproto>=0.14->trio-websocket~=0.9->selenium) (0.14.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install selenium"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cd4312a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import selenium\n",
    "import pandas as pd\n",
    "from selenium import webdriver\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "from selenium.common.exceptions import StaleElementReferenceException, NoSuchElementException\n",
    "from selenium.webdriver.common.by import By\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f6aa2758",
   "metadata": {},
   "outputs": [],
   "source": [
    "driver = webdriver.Chrome(r\"C:\\Users\\mysel\\Downloads\\chromedriver_win32\\chromedriver.exe\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "28880b3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "driver=webdriver.Chrome(r\"chromedriver.exe\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3efa9385",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Question 1: Write a python program to scrape data for “Data Analyst” Job position in “Bangalore” location. You\n",
    "#have to scrape the job-title, job-location, company_name, experience_required. You have to scrape first 10\n",
    "#jobs data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b8216059",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "driver.get(\"https://www.naukri.com\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "24b41738",
   "metadata": {},
   "outputs": [],
   "source": [
    "designation = driver.find_element(By.CLASS_NAME,\"suggestor-input\")\n",
    "designation.send_keys(\"Data Analyst\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "39d63704",
   "metadata": {},
   "outputs": [],
   "source": [
    "location = driver.find_element(By.XPATH,\"/html/body/div[1]/div[6]/div/div/div[5]/div/div/div/input\")\n",
    "location.send_keys(\"Bangalore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9ae98100",
   "metadata": {},
   "outputs": [],
   "source": [
    "search = driver.find_element(By.CLASS_NAME,\"qsbSubmit\")\n",
    "search.click()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3679e9e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "job_title=[]\n",
    "job_location=[]\n",
    "company_name=[]\n",
    "experience_required=[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "83c08a5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "title_tags=driver.find_elements(By.XPATH,'//a[@class=\"title ellipsis\"]')\n",
    "for i in title_tags[0:10]:\n",
    "    title=i.text\n",
    "    job_title.append(title)\n",
    "    \n",
    "location_tags=driver.find_elements(By.XPATH,'//span[@class=\"ellipsis fleft locWdth\"]')\n",
    "for i in location_tags[0:10]:\n",
    "    location=i.text\n",
    "    job_location.append(location)\n",
    "    \n",
    "company_tags=driver.find_elements(By.XPATH,'//a[@class=\"subTitle ellipsis fleft\"]')\n",
    "for i in company_tags[0:10]:\n",
    "    company=i.text\n",
    "    company_name.append(company)\n",
    "    \n",
    "experience_tags=driver.find_elements(By.XPATH,'//div[@class=\"ellipsis job-description\"]')\n",
    "for i in experience_tags[0:10]:\n",
    "    experience=i.text\n",
    "    experience_required.append(experience)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4cd9e413",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0 10 10\n"
     ]
    }
   ],
   "source": [
    "print(len(job_title),len(job_location),len(company_name),len(experience_required))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ea22368",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df1=pd.DataFrame({\"Title\":job_title,\"Location\":job_location,\"Company_name\":company_name,\"Experience\":experience_required })\n",
    "df1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0b0e97d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Q2:Write a python program to scrape data for “Data Scientist” Job position in “Bangalore” location. You\n",
    "#have to scrape the job-title, job-location, company_name. You have to scrape first 10 jobs data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "bed5b682",
   "metadata": {},
   "outputs": [],
   "source": [
    "driver = webdriver.Chrome(r\"C:\\Users\\mysel\\Downloads\\chromedriver_win32\\chromedriver.exe\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5e2dd07c",
   "metadata": {},
   "outputs": [],
   "source": [
    "driver=webdriver.Chrome(r\"chromedriver.exe\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "10cb1a25",
   "metadata": {},
   "outputs": [],
   "source": [
    "driver.get(\"https://www.naukri.com\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "56929f7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "designation2 = driver.find_element(By.CLASS_NAME,\"suggestor-input\")\n",
    "designation2.send_keys(\"Data scientist\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "be126a72",
   "metadata": {},
   "outputs": [],
   "source": [
    "location2 = driver.find_element(By.XPATH,\"/html/body/div[1]/div[6]/div/div/div[5]/div/div/div/input\")\n",
    "location2.send_keys(\"Bangalore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "58def696",
   "metadata": {},
   "outputs": [],
   "source": [
    "search2 = driver.find_element(By.CLASS_NAME,\"qsbSubmit\")\n",
    "search2.click()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "04324910",
   "metadata": {},
   "outputs": [],
   "source": [
    "job_title2=[]\n",
    "job_location2=[]\n",
    "company_name2=[]\n",
    "experience_required2=[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "e4fbb882",
   "metadata": {},
   "outputs": [],
   "source": [
    "title_tags2=driver.find_elements(By.XPATH,'//a[@class=\"title ellipsis\"]')\n",
    "for i in title_tags2[0:10]:\n",
    "    title2=i.text\n",
    "    job_title2.append(title2)\n",
    "    \n",
    "location_tags2=driver.find_elements(By.XPATH,'//span[@class=\"ellipsis fleft locWdth\"]')\n",
    "for i in location_tags2[0:10]:\n",
    "    location2=i.text\n",
    "    job_location2.append(location2)\n",
    "    \n",
    "company_tags2=driver.find_elements(By.XPATH,'//a[@class=\"subTitle ellipsis fleft\"]')\n",
    "for i in company_tags2[0:10]:\n",
    "    company2=i.text\n",
    "    company_name2.append(company2)\n",
    "    \n",
    "experience_tags2=driver.find_elements(By.XPATH,'//div[@class=\"ellipsis job-description\"]')\n",
    "for i in experience_tags2[0:10]:\n",
    "    experience2=i.text\n",
    "    experience_required2.append(experience2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "78905289",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10 10 10 10\n"
     ]
    }
   ],
   "source": [
    "print(len(job_title2),len(job_location2),len(company_name2),len(experience_required2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "36bce71c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Title</th>\n",
       "      <th>Location</th>\n",
       "      <th>Company_name</th>\n",
       "      <th>Experience</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Data Science Specialist</td>\n",
       "      <td>Bangalore/Bengaluru, Kolkata, Mumbai, Hyderaba...</td>\n",
       "      <td>Accenture</td>\n",
       "      <td>Must Have Knowledge of statistical techniques ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Analystics &amp; Modeling Specialist</td>\n",
       "      <td>Bangalore/Bengaluru, Kolkata, Mumbai, Hyderaba...</td>\n",
       "      <td>Accenture</td>\n",
       "      <td>This role will be a part of Survey Solutions a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Data Scientist</td>\n",
       "      <td>Hybrid - Bangalore/Bengaluru, Kolkata, Hyderab...</td>\n",
       "      <td>Cognizant</td>\n",
       "      <td>Good amount of hands-on experience in Machine ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Staff Data Scientist</td>\n",
       "      <td>Bangalore/Bengaluru, Mumbai</td>\n",
       "      <td>Baker Hughes</td>\n",
       "      <td>Independently deliver guide the team on ambigu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Sr. Data scientist</td>\n",
       "      <td>Bangalore/Bengaluru, Hyderabad/Secunderabad, M...</td>\n",
       "      <td>Tata Consultancy Services (TCS)</td>\n",
       "      <td>Roles and Responsibilities 7+ yrs of experienc...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Data Scientist II</td>\n",
       "      <td>Bangalore/Bengaluru</td>\n",
       "      <td>Uber</td>\n",
       "      <td>. What You Will Need . . . . MS or Bachelors d...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Physicist / Scientist</td>\n",
       "      <td>Bangalore/Bengaluru</td>\n",
       "      <td>Applied Materials</td>\n",
       "      <td>Support physics-related application and system...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Senior Data Scientist</td>\n",
       "      <td>Bangalore/Bengaluru, Kolkata, Mumbai, New Delh...</td>\n",
       "      <td>Justified Advertising Software Services India ...</td>\n",
       "      <td>Evaluating the statistical methods and procedu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Principal Data Scientist</td>\n",
       "      <td>Bangalore/Bengaluru, Kolkata, Mumbai, New Delh...</td>\n",
       "      <td>Agilon Health</td>\n",
       "      <td>Experience training and deploying models in a ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Expert Data Scientist</td>\n",
       "      <td>Bangalore/Bengaluru, Mumbai</td>\n",
       "      <td>UPL Limited</td>\n",
       "      <td>The Data Science Lead will use both management...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                              Title  \\\n",
       "0           Data Science Specialist   \n",
       "1  Analystics & Modeling Specialist   \n",
       "2                    Data Scientist   \n",
       "3              Staff Data Scientist   \n",
       "4                Sr. Data scientist   \n",
       "5                 Data Scientist II   \n",
       "6             Physicist / Scientist   \n",
       "7             Senior Data Scientist   \n",
       "8          Principal Data Scientist   \n",
       "9             Expert Data Scientist   \n",
       "\n",
       "                                            Location  \\\n",
       "0  Bangalore/Bengaluru, Kolkata, Mumbai, Hyderaba...   \n",
       "1  Bangalore/Bengaluru, Kolkata, Mumbai, Hyderaba...   \n",
       "2  Hybrid - Bangalore/Bengaluru, Kolkata, Hyderab...   \n",
       "3                        Bangalore/Bengaluru, Mumbai   \n",
       "4  Bangalore/Bengaluru, Hyderabad/Secunderabad, M...   \n",
       "5                                Bangalore/Bengaluru   \n",
       "6                                Bangalore/Bengaluru   \n",
       "7  Bangalore/Bengaluru, Kolkata, Mumbai, New Delh...   \n",
       "8  Bangalore/Bengaluru, Kolkata, Mumbai, New Delh...   \n",
       "9                        Bangalore/Bengaluru, Mumbai   \n",
       "\n",
       "                                        Company_name  \\\n",
       "0                                          Accenture   \n",
       "1                                          Accenture   \n",
       "2                                          Cognizant   \n",
       "3                                       Baker Hughes   \n",
       "4                    Tata Consultancy Services (TCS)   \n",
       "5                                               Uber   \n",
       "6                                  Applied Materials   \n",
       "7  Justified Advertising Software Services India ...   \n",
       "8                                      Agilon Health   \n",
       "9                                        UPL Limited   \n",
       "\n",
       "                                          Experience  \n",
       "0  Must Have Knowledge of statistical techniques ...  \n",
       "1  This role will be a part of Survey Solutions a...  \n",
       "2  Good amount of hands-on experience in Machine ...  \n",
       "3  Independently deliver guide the team on ambigu...  \n",
       "4  Roles and Responsibilities 7+ yrs of experienc...  \n",
       "5  . What You Will Need . . . . MS or Bachelors d...  \n",
       "6  Support physics-related application and system...  \n",
       "7  Evaluating the statistical methods and procedu...  \n",
       "8  Experience training and deploying models in a ...  \n",
       "9  The Data Science Lead will use both management...  "
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df2=pd.DataFrame({\"Title\":job_title2,\"Location\":job_location2,\"Company_name\":company_name2,\"Experience\":experience_required2})\n",
    "df2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "51159a75",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Question 3: In this question you have to scrape data using the filters available on the webpage as shown below:\n",
    "#You have to use the location and salary filter.\n",
    "#You have to scrape data for “Data Scientist” designation for first 10 job results.\n",
    "#You have to scrape the job-title, job-location, company name, experience required.\n",
    "#The location filter to be used is “Delhi/NCR”. The salary filter to be used is “3-6” lakhs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "a95db1ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "driver = webdriver.Chrome(r\"C:\\Users\\mysel\\Downloads\\chromedriver_win32\\chromedriver.exe\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "4908772f",
   "metadata": {},
   "outputs": [],
   "source": [
    "driver=webdriver.Chrome(r\"chromedriver.exe\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "4936db15",
   "metadata": {},
   "outputs": [],
   "source": [
    "driver.get(\"https://www.naukri.com\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "735d9b76",
   "metadata": {},
   "outputs": [],
   "source": [
    "designation3 = driver.find_element(By.CLASS_NAME,\"suggestor-input\")\n",
    "designation3.send_keys(\"Data scientist\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "0b7ce57c",
   "metadata": {},
   "outputs": [],
   "source": [
    "search3 = driver.find_element(By.CLASS_NAME,\"qsbSubmit\")\n",
    "search3.click()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "6eab5860",
   "metadata": {},
   "outputs": [],
   "source": [
    "location3 = driver.find_element(By.XPATH,\"/html/body/div[1]/div[4]/div/div/section[1]/div[2]/div[5]/div[2]/div[2]/label/i\")\n",
    "location3.click()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "fe370ec7",
   "metadata": {},
   "outputs": [],
   "source": [
    "salary3 = driver.find_element(By.XPATH,\"/html/body/div[1]/div[4]/div/div/section[1]/div[2]/div[6]/div[2]/div[2]/label/i\")\n",
    "salary3.click()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "0c639216",
   "metadata": {},
   "outputs": [],
   "source": [
    "job_title3=[]\n",
    "job_location3=[]\n",
    "company_name3=[]\n",
    "experience_required3=[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "0c0a20fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "title_tags3=driver.find_elements(By.XPATH,'//a[@class=\"title ellipsis\"]')\n",
    "for i in title_tags3[0:10]:\n",
    "    title3=i.text\n",
    "    job_title3.append(title3)\n",
    "    \n",
    "location_tags3=driver.find_elements(By.XPATH,'//span[@class=\"ellipsis fleft locWdth\"]')\n",
    "for i in location_tags3[0:10]:\n",
    "    location3=i.text\n",
    "    job_location3.append(location3)\n",
    "    \n",
    "company_tags3=driver.find_elements(By.XPATH,'//a[@class=\"subTitle ellipsis fleft\"]')\n",
    "for i in company_tags3[0:10]:\n",
    "    company3=i.text\n",
    "    company_name3.append(company3)\n",
    "    \n",
    "experience_tags3=driver.find_elements(By.XPATH,'//div[@class=\"ellipsis job-description\"]')\n",
    "for i in experience_tags3[0:10]:\n",
    "    experience3=i.text\n",
    "    experience_required3.append(experience3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "2e72ad8a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10 10 10 10\n"
     ]
    }
   ],
   "source": [
    "print(len(job_title3),len(job_location3),len(company_name3),len(experience_required3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "63069ed2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Title</th>\n",
       "      <th>Location</th>\n",
       "      <th>Company_name</th>\n",
       "      <th>Experience</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Data Scientist data Analyst</td>\n",
       "      <td>Delhi / NCR, Kolkata, Hyderabad/Secunderabad, ...</td>\n",
       "      <td>Smark Laser Automation</td>\n",
       "      <td>Participate in the entire application lifecycl...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Data Scientist - Engine Algorithm</td>\n",
       "      <td>Delhi / NCR, Kolkata, Mumbai, Hyderabad/Secund...</td>\n",
       "      <td>Primo Hiring</td>\n",
       "      <td>We are looking for a Data Scientist, who will ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Analyst-Data Science</td>\n",
       "      <td>Gurgaon/Gurugram</td>\n",
       "      <td>AMERICAN EXPRESS</td>\n",
       "      <td>. Engg, MBA, Master s Degree in Economics, Sta...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Analyst-Data Science</td>\n",
       "      <td>Gurgaon/Gurugram</td>\n",
       "      <td>AMERICAN EXPRESS</td>\n",
       "      <td>. . . . . Engineering, MBA, Master s Degree in...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Senior Data Scientist - Statistics</td>\n",
       "      <td>Gurgaon/Gurugram</td>\n",
       "      <td>Optum</td>\n",
       "      <td>Timely and quality delivery of engagements wit...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Data Scientist</td>\n",
       "      <td>Delhi / NCR, Noida(Sector-136 Noida), Ghaziaba...</td>\n",
       "      <td>Extramarks Education</td>\n",
       "      <td>Should have prior understanding or working exp...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Data Scientist || Walk-In || 5th-6th Jan</td>\n",
       "      <td>Hybrid - Noida(Sector-62 Noida)</td>\n",
       "      <td>SEW Engineering</td>\n",
       "      <td>Masters degree in Machine Learning, Data Scien...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Research Scientist - Bioinformatics</td>\n",
       "      <td>Gurgaon/Gurugram</td>\n",
       "      <td>Biopeople India</td>\n",
       "      <td>Experience in molecular biology and associated...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Data Scientist</td>\n",
       "      <td>Gurgaon/Gurugram</td>\n",
       "      <td>Gartner India Research &amp;amp; Advisory Services...</td>\n",
       "      <td>Experience in handling unstructured data and a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Associate Data Scientist - ML &amp; NLP</td>\n",
       "      <td>Gurgaon/Gurugram</td>\n",
       "      <td>Gartner India Research &amp;amp; Advisory Services...</td>\n",
       "      <td>Bachelor s or Masters degree in Physics, Math,...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                      Title  \\\n",
       "0               Data Scientist data Analyst   \n",
       "1         Data Scientist - Engine Algorithm   \n",
       "2                      Analyst-Data Science   \n",
       "3                      Analyst-Data Science   \n",
       "4        Senior Data Scientist - Statistics   \n",
       "5                            Data Scientist   \n",
       "6  Data Scientist || Walk-In || 5th-6th Jan   \n",
       "7       Research Scientist - Bioinformatics   \n",
       "8                            Data Scientist   \n",
       "9       Associate Data Scientist - ML & NLP   \n",
       "\n",
       "                                            Location  \\\n",
       "0  Delhi / NCR, Kolkata, Hyderabad/Secunderabad, ...   \n",
       "1  Delhi / NCR, Kolkata, Mumbai, Hyderabad/Secund...   \n",
       "2                                   Gurgaon/Gurugram   \n",
       "3                                   Gurgaon/Gurugram   \n",
       "4                                   Gurgaon/Gurugram   \n",
       "5  Delhi / NCR, Noida(Sector-136 Noida), Ghaziaba...   \n",
       "6                    Hybrid - Noida(Sector-62 Noida)   \n",
       "7                                   Gurgaon/Gurugram   \n",
       "8                                   Gurgaon/Gurugram   \n",
       "9                                   Gurgaon/Gurugram   \n",
       "\n",
       "                                        Company_name  \\\n",
       "0                             Smark Laser Automation   \n",
       "1                                       Primo Hiring   \n",
       "2                                   AMERICAN EXPRESS   \n",
       "3                                   AMERICAN EXPRESS   \n",
       "4                                              Optum   \n",
       "5                               Extramarks Education   \n",
       "6                                    SEW Engineering   \n",
       "7                                    Biopeople India   \n",
       "8  Gartner India Research &amp; Advisory Services...   \n",
       "9  Gartner India Research &amp; Advisory Services...   \n",
       "\n",
       "                                          Experience  \n",
       "0  Participate in the entire application lifecycl...  \n",
       "1  We are looking for a Data Scientist, who will ...  \n",
       "2  . Engg, MBA, Master s Degree in Economics, Sta...  \n",
       "3  . . . . . Engineering, MBA, Master s Degree in...  \n",
       "4  Timely and quality delivery of engagements wit...  \n",
       "5  Should have prior understanding or working exp...  \n",
       "6  Masters degree in Machine Learning, Data Scien...  \n",
       "7  Experience in molecular biology and associated...  \n",
       "8  Experience in handling unstructured data and a...  \n",
       "9  Bachelor s or Masters degree in Physics, Math,...  "
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df3=pd.DataFrame({\"Title\":job_title3,\"Location\":job_location3,\"Company_name\":company_name3,\"Experience\":experience_required3})\n",
    "df3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "49e545fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Question 4: Scrape data of first 100 sunglasses listings on flipkart.com. You have to scrape four attributes:\n",
    "#1. Brand\n",
    "#2. ProductDescription\n",
    "#3. Price\n",
    "#The attributes which you have to scrape is ticked marked in the below image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "f578a454",
   "metadata": {},
   "outputs": [],
   "source": [
    "driver = webdriver.Chrome(r\"C:\\Users\\mysel\\Downloads\\chromedriver_win32\\chromedriver.exe\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "1d8e7f80",
   "metadata": {},
   "outputs": [],
   "source": [
    "driver=webdriver.Chrome(r\"chromedriver.exe\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "3881dc4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "driver.get(\"https://www.flipkart.com/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "23ca86f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "cross4 = driver.find_element(By.XPATH,\"/html/body/div[2]/div/div/button\")\n",
    "cross4.click()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "62ad8a0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "product4 = driver.find_element(By.XPATH,\"/html/body/div[1]/div/div[1]/div[1]/div[2]/div[2]/form/div/div/input\")\n",
    "product4.send_keys(\"sunglasses\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "60bbe36c",
   "metadata": {},
   "outputs": [],
   "source": [
    "search4 = driver.find_element(By.XPATH,\"/html/body/div[1]/div/div[1]/div[1]/div[2]/div[2]/form/div/button\")\n",
    "search4.click()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "6d976158",
   "metadata": {},
   "outputs": [],
   "source": [
    "sunglass_brand4=[]\n",
    "sunglass_product_description4=[]\n",
    "sunglass_price4=[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "9fbcbfdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "brand_tags4=driver.find_elements(By.XPATH,'//div[@class=\"_2WkVRV\"]')\n",
    "for i in brand_tags4[0:100]:\n",
    "    brand4=i.text\n",
    "    sunglass_brand4.append(brand4)\n",
    "    \n",
    "product_description_tags4=driver.find_elements(By.XPATH,'//a[@class=\"IRpwTa\"]')\n",
    "for i in product_description_tags4[0:100]:\n",
    "    product4=i.text\n",
    "    sunglass_product_description4.append(product4)\n",
    "    \n",
    "price_tags4=driver.find_elements(By.XPATH,'//div[@class=\"_30jeq3\"]')\n",
    "for i in price_tags4[0:100]:\n",
    "    price4=i.text\n",
    "    sunglass_price4.append(price4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "adb8ef90",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "40 40 40\n"
     ]
    }
   ],
   "source": [
    "print(len(sunglass_brand4),len(sunglass_product_description4),len(sunglass_price4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "1eaccc7b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Brand</th>\n",
       "      <th>Product_Description</th>\n",
       "      <th>Price</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Elligator</td>\n",
       "      <td>UV Protection Cat-eye, Retro Square, Oval, Rou...</td>\n",
       "      <td>₹129</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Elligator</td>\n",
       "      <td>UV Protection Cat-eye, Retro Square, Oval, Rou...</td>\n",
       "      <td>₹129</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Elligator</td>\n",
       "      <td>UV Protection Cat-eye, Retro Square, Oval, Rou...</td>\n",
       "      <td>₹149</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Fastrack</td>\n",
       "      <td>UV Protection Rectangular Sunglasses (Free Size)</td>\n",
       "      <td>₹519</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Fastrack</td>\n",
       "      <td>Gradient, UV Protection Wayfarer Sunglasses (F...</td>\n",
       "      <td>₹579</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       Brand                                Product_Description Price\n",
       "0  Elligator  UV Protection Cat-eye, Retro Square, Oval, Rou...  ₹129\n",
       "1  Elligator  UV Protection Cat-eye, Retro Square, Oval, Rou...  ₹129\n",
       "2  Elligator  UV Protection Cat-eye, Retro Square, Oval, Rou...  ₹149\n",
       "3   Fastrack   UV Protection Rectangular Sunglasses (Free Size)  ₹519\n",
       "4   Fastrack  Gradient, UV Protection Wayfarer Sunglasses (F...  ₹579"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df4=pd.DataFrame({\"Brand\":sunglass_brand4,\"Product_Description\":sunglass_product_description4,\"Price\":sunglass_price4})\n",
    "df4.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "27f307d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Question 5: Scrape 100 reviews data from flipkart.com for iphone11 phone. You have to go the link:\n",
    "#https://www.flipkart.com/apple-iphone-11-black-64-gb/productreviews/itm4e5041ba101fd?pid=MOBFWQ6BXGJCEYNY&lid=LSTMOBFWQ6BXGJCEYNYZXSHRJ&market\n",
    "#place=FLIPKART\n",
    "#As shown in the above page you have to scrape the tick marked attributes. These are:\n",
    "#1. Rating\n",
    "#2. Review summary\n",
    "#3. Full review\n",
    "#4. You have to scrape this data for first 100reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "e68d5cc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "driver = webdriver.Chrome(r\"C:\\Users\\mysel\\Downloads\\chromedriver_win32\\chromedriver.exe\")\n",
    "driver=webdriver.Chrome(r\"chromedriver.exe\")\n",
    "driver.get(\"https://www.flipkart.com/apple-iphone-11-black-64-gb/product-reviews/itm4e5041ba101fd?pid=MOBFWQ6BXGJCEYNY&lid=LSTMOBFWQ6BXGJCEYNYZXSHRJ&market\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "1969c294",
   "metadata": {},
   "outputs": [],
   "source": [
    "iphone_rating5=[]\n",
    "iphone_review_summary5=[]\n",
    "iphone_full_review5=[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "ad5f981b",
   "metadata": {},
   "outputs": [],
   "source": [
    "rating_tags5=driver.find_elements(By.XPATH,'//div[@class=\"_3LWZlK _1BLPMq\"]')\n",
    "for i in rating_tags5[0:100]:\n",
    "    rating5=i.text\n",
    "    iphone_rating5.append(rating5)\n",
    "    \n",
    "review_summary_tags5=driver.find_elements(By.XPATH,'//a[@class=\"_2-N8zT\"]')\n",
    "for i in review_summary_tags5[0:100]:\n",
    "    review5=i.text\n",
    "    iphone_review_summary5.append(review5)\n",
    "    \n",
    "full_review_tags5=driver.find_elements(By.XPATH,'//div[@class=\"_30jeq3\"]')\n",
    "for i in full_review_tags5[0:100]:\n",
    "    full_review5=i.text\n",
    "    iphone_full_review5.append(full_review5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "b1dcff50",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10 0 1\n"
     ]
    }
   ],
   "source": [
    "print(len(iphone_rating5),len(iphone_review_summary5),len(iphone_full_review5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "e248704a",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "All arrays must be of the same length",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Input \u001b[1;32mIn [132]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[0m df5\u001b[38;5;241m=\u001b[39m\u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mDataFrame\u001b[49m\u001b[43m(\u001b[49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mRating\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43miphone_rating5\u001b[49m\u001b[43m,\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mReview summary\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43miphone_review_summary5\u001b[49m\u001b[43m,\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mFull Review\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43miphone_full_review5\u001b[49m\u001b[43m,\u001b[49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      2\u001b[0m df5\u001b[38;5;241m.\u001b[39mhead()\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\core\\frame.py:636\u001b[0m, in \u001b[0;36mDataFrame.__init__\u001b[1;34m(self, data, index, columns, dtype, copy)\u001b[0m\n\u001b[0;32m    630\u001b[0m     mgr \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_init_mgr(\n\u001b[0;32m    631\u001b[0m         data, axes\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mindex\u001b[39m\u001b[38;5;124m\"\u001b[39m: index, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcolumns\u001b[39m\u001b[38;5;124m\"\u001b[39m: columns}, dtype\u001b[38;5;241m=\u001b[39mdtype, copy\u001b[38;5;241m=\u001b[39mcopy\n\u001b[0;32m    632\u001b[0m     )\n\u001b[0;32m    634\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data, \u001b[38;5;28mdict\u001b[39m):\n\u001b[0;32m    635\u001b[0m     \u001b[38;5;66;03m# GH#38939 de facto copy defaults to False only in non-dict cases\u001b[39;00m\n\u001b[1;32m--> 636\u001b[0m     mgr \u001b[38;5;241m=\u001b[39m \u001b[43mdict_to_mgr\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindex\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcolumns\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcopy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcopy\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtyp\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmanager\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    637\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data, ma\u001b[38;5;241m.\u001b[39mMaskedArray):\n\u001b[0;32m    638\u001b[0m     \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mma\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmrecords\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mmrecords\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\core\\internals\\construction.py:502\u001b[0m, in \u001b[0;36mdict_to_mgr\u001b[1;34m(data, index, columns, dtype, typ, copy)\u001b[0m\n\u001b[0;32m    494\u001b[0m     arrays \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m    495\u001b[0m         x\n\u001b[0;32m    496\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(x, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdtype\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(x\u001b[38;5;241m.\u001b[39mdtype, ExtensionDtype)\n\u001b[0;32m    497\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m x\u001b[38;5;241m.\u001b[39mcopy()\n\u001b[0;32m    498\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m arrays\n\u001b[0;32m    499\u001b[0m     ]\n\u001b[0;32m    500\u001b[0m     \u001b[38;5;66;03m# TODO: can we get rid of the dt64tz special case above?\u001b[39;00m\n\u001b[1;32m--> 502\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43marrays_to_mgr\u001b[49m\u001b[43m(\u001b[49m\u001b[43marrays\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcolumns\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindex\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtyp\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtyp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconsolidate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcopy\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\core\\internals\\construction.py:120\u001b[0m, in \u001b[0;36marrays_to_mgr\u001b[1;34m(arrays, columns, index, dtype, verify_integrity, typ, consolidate)\u001b[0m\n\u001b[0;32m    117\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m verify_integrity:\n\u001b[0;32m    118\u001b[0m     \u001b[38;5;66;03m# figure out the index, if necessary\u001b[39;00m\n\u001b[0;32m    119\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m index \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 120\u001b[0m         index \u001b[38;5;241m=\u001b[39m \u001b[43m_extract_index\u001b[49m\u001b[43m(\u001b[49m\u001b[43marrays\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    121\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    122\u001b[0m         index \u001b[38;5;241m=\u001b[39m ensure_index(index)\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\core\\internals\\construction.py:674\u001b[0m, in \u001b[0;36m_extract_index\u001b[1;34m(data)\u001b[0m\n\u001b[0;32m    672\u001b[0m lengths \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mset\u001b[39m(raw_lengths))\n\u001b[0;32m    673\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(lengths) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m--> 674\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAll arrays must be of the same length\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    676\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m have_dicts:\n\u001b[0;32m    677\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    678\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMixing dicts with non-Series may lead to ambiguous ordering.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    679\u001b[0m     )\n",
      "\u001b[1;31mValueError\u001b[0m: All arrays must be of the same length"
     ]
    }
   ],
   "source": [
    "df5=pd.DataFrame({\"Rating\":iphone_rating5,\"Review summary\":iphone_review_summary5,\"Full Review\":iphone_full_review5})\n",
    "df5.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "9d04a726",
   "metadata": {},
   "outputs": [],
   "source": [
    "#QUestion 6: Scrape data for first 100 sneakers you find when you visit flipkart.com and search for “sneakers” in the\n",
    "#search field.\n",
    "#You have to scrape 3 attributes of each sneaker\n",
    "#1. Brand\n",
    "#2. ProductDescription\n",
    "#3. Price"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "4df1ed4d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0 0\n"
     ]
    }
   ],
   "source": [
    "driver = webdriver.Chrome(r\"C:\\Users\\mysel\\Downloads\\chromedriver_win32\\chromedriver.exe\")\n",
    "driver=webdriver.Chrome(r\"chromedriver.exe\")\n",
    "driver.get(\"https://www.flipkart.com/\")\n",
    "cross6 = driver.find_element(By.XPATH,\"/html/body/div[2]/div/div/button\")\n",
    "cross6.click()\n",
    "product6 = driver.find_element(By.XPATH,\"/html/body/div[1]/div/div[1]/div[1]/div[2]/div[2]/form/div/div/input\")\n",
    "product6.send_keys(\"sneakers\")\n",
    "search4 = driver.find_element(By.XPATH,\"/html/body/div[1]/div/div[1]/div[1]/div[2]/div[2]/form/div/button\")\n",
    "search4.click()\n",
    "\n",
    "sneakers_brand6=[]\n",
    "sneakers_product_description6=[]\n",
    "sneakers_price6=[]\n",
    "\n",
    "brand_tags6=driver.find_elements(By.XPATH,'//div[@class=\"_2WkVRV\"]')\n",
    "for i in brand_tags6[0:100]:\n",
    "    brand6=i.text\n",
    "    sneakers_brand6.append(brand6)\n",
    "    \n",
    "product_description_tags6=driver.find_elements(By.XPATH,'//a[@class=\"IRpwTa\"]')\n",
    "for i in product_description_tags6[0:100]:\n",
    "    product6=i.text\n",
    "    sneakers_product_description6.append(product6)\n",
    "    \n",
    "price_tags6=driver.find_elements(By.XPATH,'//div[@class=\"_30jeq3\"]')\n",
    "for i in price_tags6[0:100]:\n",
    "    price6=i.text\n",
    "    sneakers_price6.append(price6)\n",
    "    \n",
    "print(len(sneakers_brand6),len(sneakers_product_description6),len(sneakers_price6))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "2535817c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Brand</th>\n",
       "      <th>Product_Description</th>\n",
       "      <th>Price</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [Brand, Product_Description, Price]\n",
       "Index: []"
      ]
     },
     "execution_count": 139,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df6=pd.DataFrame({\"Brand\":sneakers_brand6,\"Product_Description\":sneakers_product_description6,\"Price\":sneakers_price6})\n",
    "df6.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "561c99ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Question 7: Go to webpage https://www.amazon.in/ Enter “Laptop” in the search field and then click the search icon. Then\n",
    "#set CPU Type filter to “Intel Core i7” as shown in the below image:\n",
    "#After setting the filters scrape first 10 laptops data. You have to scrape 3 attributes for each laptop:\n",
    "#1. Title\n",
    "#2. Ratings\n",
    "#3. Price"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "068489ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0 0\n"
     ]
    }
   ],
   "source": [
    "driver = webdriver.Chrome(r\"C:\\Users\\mysel\\Downloads\\chromedriver_win32\\chromedriver.exe\")\n",
    "driver=webdriver.Chrome(r\"chromedriver.exe\")\n",
    "driver.get(\"https://www.amazon.in/\")\n",
    "\n",
    "product7 = driver.find_element(By.XPATH,\"/html/body/div[1]/header/div/div[1]/div[2]/div/form/div[2]/div[1]/input\")\n",
    "product7.send_keys(\"laptop\")\n",
    "\n",
    "search7 = driver.find_element(By.XPATH,\"/html/body/div[1]/header/div/div[1]/div[2]/div/form/div[3]/div/span/input\")\n",
    "search7.click()\n",
    "\n",
    "filter7 = driver.find_element(By.XPATH,\"/html/body/div[1]/div[2]/div[1]/div[2]/div/div[3]/span/div[1]/div/div/div[6]/ul[6]/li[9]/span/a/div/label/i\")\n",
    "filter7.click()\n",
    "\n",
    "laptop_title7=[]\n",
    "laptop_ratings7=[]\n",
    "laptop_price7=[]\n",
    "\n",
    "title_tags7=driver.find_elements(By.XPATH,'//div[@class=\"a-size-medium a-color-base a-text-normal\"]')\n",
    "for i in title_tags7[0:100]:\n",
    "    title7=i.text\n",
    "    laptop_title7.append(title7)\n",
    "    \n",
    "ratings_tags7=driver.find_elements(By.XPATH,'//a[@class=\"a-size-base\"]')\n",
    "for i in ratings_tags7[0:100]:\n",
    "    ratings7=i.text\n",
    "    laptop_ratings7.append(ratings7)\n",
    "    \n",
    "price_tags7=driver.find_elements(By.XPATH,'//div[@class=\"_30jeq3\"]')\n",
    "for i in price_tags7[0:100]:\n",
    "    price7=i.text\n",
    "    laptop_price7.append(price7)\n",
    "    \n",
    "print(len(laptop_title7),len(laptop_ratings7),len(laptop_price7))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "edfbf69c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Brand</th>\n",
       "      <th>Product_Description</th>\n",
       "      <th>Price</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [Brand, Product_Description, Price]\n",
       "Index: []"
      ]
     },
     "execution_count": 142,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df7=pd.DataFrame({\"Title\":laptop_title7,\"Ratings\":laptop_ratings7,\"Price\":laptop_price7})\n",
    "df7.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "6aa80248",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Question 8: Write a python program to scrape data for Top 1000 Quotes of All Time.\n",
    "#The above task will be done in following steps:\n",
    "#1. First get the webpagehttps://www.azquotes.com/\n",
    "#2. Click on TopQuotes\n",
    "#3. Than scrap a) Quote b) Author c) Type Of Quotes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "ecce67da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4 0 100\n"
     ]
    }
   ],
   "source": [
    "driver = webdriver.Chrome(r\"C:\\Users\\mysel\\Downloads\\chromedriver_win32\\chromedriver.exe\")\n",
    "driver=webdriver.Chrome(r\"chromedriver.exe\")\n",
    "driver.get(\"https://www.azquotes.com/\")\n",
    "\n",
    "top_quote8 = driver.find_element(By.XPATH,\"/html/body/div[1]/div[1]/div[1]/div/div[3]/ul/li[5]/a\")\n",
    "top_quote8.click()\n",
    "\n",
    "top_quote8=[]\n",
    "top_author8=[]\n",
    "top_type_of_quotes8=[]\n",
    "\n",
    "quote_tags8=driver.find_elements(By.XPATH,'//div[@class=\"title\"]')\n",
    "for i in quote_tags8[0:1000]:\n",
    "    quote8=i.text\n",
    "    top_quote8.append(quote8)\n",
    "    \n",
    "author_tags8=driver.find_elements(By.XPATH,'//a[@class=\"author\"]')\n",
    "for i in author_tags8[0:1000]:\n",
    "    author8=i.text\n",
    "    top_author8.append(author8)\n",
    "    \n",
    "type_of_quotes_tags8=driver.find_elements(By.XPATH,'//div[@class=\"tags\"]')\n",
    "for i in type_of_quotes_tags8[0:1000]:\n",
    "    type_of_quotes8=i.text\n",
    "    top_type_of_quotes8.append(type_of_quotes8)\n",
    "    \n",
    "print(len(top_quote8),len(top_author8),len(top_type_of_quotes8))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "dc1f2468",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "All arrays must be of the same length",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Input \u001b[1;32mIn [146]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[0m df8\u001b[38;5;241m=\u001b[39m\u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mDataFrame\u001b[49m\u001b[43m(\u001b[49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mBrand\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43mtop_quote8\u001b[49m\u001b[43m,\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mProduct_Description\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43mtop_author8\u001b[49m\u001b[43m,\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mPrice\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43mtop_type_of_quotes8\u001b[49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      2\u001b[0m df8\u001b[38;5;241m.\u001b[39mhead()\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\core\\frame.py:636\u001b[0m, in \u001b[0;36mDataFrame.__init__\u001b[1;34m(self, data, index, columns, dtype, copy)\u001b[0m\n\u001b[0;32m    630\u001b[0m     mgr \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_init_mgr(\n\u001b[0;32m    631\u001b[0m         data, axes\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mindex\u001b[39m\u001b[38;5;124m\"\u001b[39m: index, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcolumns\u001b[39m\u001b[38;5;124m\"\u001b[39m: columns}, dtype\u001b[38;5;241m=\u001b[39mdtype, copy\u001b[38;5;241m=\u001b[39mcopy\n\u001b[0;32m    632\u001b[0m     )\n\u001b[0;32m    634\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data, \u001b[38;5;28mdict\u001b[39m):\n\u001b[0;32m    635\u001b[0m     \u001b[38;5;66;03m# GH#38939 de facto copy defaults to False only in non-dict cases\u001b[39;00m\n\u001b[1;32m--> 636\u001b[0m     mgr \u001b[38;5;241m=\u001b[39m \u001b[43mdict_to_mgr\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindex\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcolumns\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcopy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcopy\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtyp\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmanager\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    637\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data, ma\u001b[38;5;241m.\u001b[39mMaskedArray):\n\u001b[0;32m    638\u001b[0m     \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mma\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmrecords\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mmrecords\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\core\\internals\\construction.py:502\u001b[0m, in \u001b[0;36mdict_to_mgr\u001b[1;34m(data, index, columns, dtype, typ, copy)\u001b[0m\n\u001b[0;32m    494\u001b[0m     arrays \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m    495\u001b[0m         x\n\u001b[0;32m    496\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(x, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdtype\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(x\u001b[38;5;241m.\u001b[39mdtype, ExtensionDtype)\n\u001b[0;32m    497\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m x\u001b[38;5;241m.\u001b[39mcopy()\n\u001b[0;32m    498\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m arrays\n\u001b[0;32m    499\u001b[0m     ]\n\u001b[0;32m    500\u001b[0m     \u001b[38;5;66;03m# TODO: can we get rid of the dt64tz special case above?\u001b[39;00m\n\u001b[1;32m--> 502\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43marrays_to_mgr\u001b[49m\u001b[43m(\u001b[49m\u001b[43marrays\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcolumns\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindex\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtyp\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtyp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconsolidate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcopy\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\core\\internals\\construction.py:120\u001b[0m, in \u001b[0;36marrays_to_mgr\u001b[1;34m(arrays, columns, index, dtype, verify_integrity, typ, consolidate)\u001b[0m\n\u001b[0;32m    117\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m verify_integrity:\n\u001b[0;32m    118\u001b[0m     \u001b[38;5;66;03m# figure out the index, if necessary\u001b[39;00m\n\u001b[0;32m    119\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m index \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 120\u001b[0m         index \u001b[38;5;241m=\u001b[39m \u001b[43m_extract_index\u001b[49m\u001b[43m(\u001b[49m\u001b[43marrays\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    121\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    122\u001b[0m         index \u001b[38;5;241m=\u001b[39m ensure_index(index)\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\core\\internals\\construction.py:674\u001b[0m, in \u001b[0;36m_extract_index\u001b[1;34m(data)\u001b[0m\n\u001b[0;32m    672\u001b[0m lengths \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mset\u001b[39m(raw_lengths))\n\u001b[0;32m    673\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(lengths) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m--> 674\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAll arrays must be of the same length\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    676\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m have_dicts:\n\u001b[0;32m    677\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    678\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMixing dicts with non-Series may lead to ambiguous ordering.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    679\u001b[0m     )\n",
      "\u001b[1;31mValueError\u001b[0m: All arrays must be of the same length"
     ]
    }
   ],
   "source": [
    "df8=pd.DataFrame({\"Quote\":top_quote8,\"Author\":top_author8,\"Type Of Quotes\":top_type_of_quotes8})\n",
    "df8.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "064fe08a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Question 9:Write a python program to display list of respected former Prime Ministers of India(i.e. Name, Born-Dead,\n",
    "#Term of office, Remarks) from https://www.jagranjosh.com/.\n",
    "\n",
    "This task will be done in following steps:\n",
    "1. First get the webpagehttps://www.jagranjosh.com/\n",
    "2. Then You have to click on the GK option\n",
    "3. Then click on the List of all Prime Ministers of India\n",
    "4. Then scrap the mentioned data and make theDataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "17d78ad5",
   "metadata": {},
   "outputs": [],
   "source": [
    "driver = webdriver.Chrome(r\"C:\\Users\\mysel\\Downloads\\chromedriver_win32\\chromedriver.exe\")\n",
    "driver=webdriver.Chrome(r\"chromedriver.exe\")\n",
    "driver.get(\"https://www.jagranjosh.com/\")\n",
    "\n",
    "gk9 = driver.find_element(By.XPATH,\"/html/body/div/div[1]/div/div[1]/div/div[5]/div/div[1]/header/div[3]/ul/li[9]/a\")\n",
    "gk9.click()\n",
    "\n",
    "india_gk9 = driver.find_element(By.XPATH,\"/html/body/div[1]/div/div/div[2]/div/div[5]/div/div/ul/li[6]/a/span\")\n",
    "india_gk9.click()\n",
    "\n",
    "pm9 = driver.find_element(By.XPATH,\"/html/body/div[1]/div/div/div[2]/div/div[14]/div/div/ul/li[4]/a\")\n",
    "pm9.click()\n",
    "\n",
    "pm_name9=[]\n",
    "pm_born_dead9=[]\n",
    "pm_tearm_of_office9=[]\n",
    "pm_remarks9=[]\n",
    "\n",
    "#CANNOT FIND CLASS ATTRIBUTE FOR PRIME MINISTER NAMES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc73ecff",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Question 10: Write a python program to display list of 50 Most expensive cars in the world (i.e.\n",
    "#Car name and Price) from https://www.motor1.com/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb55413d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#CANNOT FIND 50 MOST EXPENSIVE CARS IN THE WORLD OPTION ON WWW.MOTOR1.COM"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
